# LLM

- Large Language Models are very large deep learning models that are pre-trained on vast amount of data.
- The underlying transformer is a set of neural networks that consists of an encoder and a decoder with self attention capabilities.
- The encoder and decoder extract the meaning from a sequence of text and understand the relationships between words and phrases it.
- Transformer LLMs are capable of unsupervised training, although a more precise explaination is that transformers perform self learning. It is through this process that transformers learn to understand basic grammar, languages and knowledge
- Transformers processes entire sequence in parallel.
- Transformer neural network architecture allows the use of very large models, often with hundreds of billions of parameters.
- Such large-scale models can ingest massive amount of data, often from the internet, but also from sources such as the common crawl, which comprises more than 50 billion web pages, and wikipedia which has approximately 57 million pages.
